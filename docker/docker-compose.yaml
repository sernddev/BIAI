version: "3"

volumes:
  data:

networks:
    app-network:
       driver: bridge
       external: true  
 
services:
  bootstrap:
    #image: ghcr.io/canner/wren-bootstrap:${WREN_BOOTSTRAP_VERSION}
    build: 
     context: ./bootstrap
     dockerfile: ./Dockerfile
    restart: on-failure
    platform: ${PLATFORM}
    environment:
      DATA_PATH: /app/data
    volumes:
      - data:/app/data
    networks:
      - app-network      
    command: /bin/sh /app/init.sh 

  wren-engine:
    #image: ghcr.io/canner/wren-engine:${WREN_ENGINE_VERSION}
    build: 
     context: ../wren-engine-0.15.1
     dockerfile: ./docker/Dockerfile
    restart: on-failure
    platform: ${PLATFORM}
    expose:
      - ${WREN_ENGINE_PORT}
      - ${WREN_ENGINE_SQL_PORT} 
    volumes:
      - ./data/wren-engine/etc:/usr/src/app/etc
      - ./data/wren-engine/:/usr/src/app
    networks:
      - app-network
    depends_on:
      - bootstrap

  ibis-server:
    #image: ghcr.io/canner/wren-engine-ibis:${IBIS_SERVER_VERSION}
    build: 
     context: ../wren-engine-0.15.1
     dockerfile: ./ibis-server/Dockerfile
    restart: on-failure
    platform: ${PLATFORM}
    expose:
      - ${IBIS_SERVER_PORT}
    environment:
      WREN_ENGINE_ENDPOINT: http://wren-engine:${WREN_ENGINE_PORT}
    networks:
      - app-network

  wren-ai-service:
    #image: ghcr.io/canner/wren-ai-service:${WREN_AI_SERVICE_VERSION}
    build: 
     context: ../wren-ai-service
    restart: on-failure
    platform: ${PLATFORM}
    expose:
      - ${WREN_AI_SERVICE_PORT}
    ports:
      - ${AI_SERVICE_FORWARD_PORT}:${WREN_AI_SERVICE_PORT}
    environment:
      # sometimes the console won't show print messages,
      # using PYTHONUNBUFFERED: 1 can fix this
      PYTHONUNBUFFERED: 1
      CONFIG_PATH: /app/data/config.yaml
    env_file:
      - ./.env
    volumes:
      - ./config.yaml:/app/data/config.yaml
    networks:
      - app-network
    depends_on:
      - qdrant

  qdrant:
    image: qdrant/qdrant:v1.11.0
    restart: on-failure
    expose:
      - 6333
      - 6334
    volumes:
      - ./data/qdrant:/qdrant/storage
    networks:
      - app-network

  wren-ui:
    #image: ghcr.io/canner/wren-ui:${WREN_UI_VERSION}
    build: 
     context: ../wren-ui
    restart: on-failure
    platform: ${PLATFORM}
    environment:
      DB_TYPE: sqlite
      # /app is the working directory in the container
      SQLITE_FILE: /app/data/db.sqlite3
      WREN_ENGINE_ENDPOINT: http://wren-engine:${WREN_ENGINE_PORT}
      WREN_AI_ENDPOINT: http://wren-ai-service:${WREN_AI_SERVICE_PORT}
      IBIS_SERVER_ENDPOINT: http://ibis-server:${IBIS_SERVER_PORT}
      # this is for telemetry to know the model, i think ai-service might be able to provide a endpoint to get the information
      GENERATION_MODEL: ${GENERATION_MODEL}
      # telemetry
      WREN_ENGINE_PORT: ${WREN_ENGINE_PORT}
      WREN_AI_SERVICE_VERSION: ${WREN_AI_SERVICE_VERSION}
      WREN_UI_VERSION: ${WREN_UI_VERSION}
      WREN_ENGINE_VERSION: ${WREN_ENGINE_VERSION}
      USER_UUID: ${USER_UUID}
      POSTHOG_API_KEY: ${POSTHOG_API_KEY}
      POSTHOG_HOST: ${POSTHOG_HOST}
      TELEMETRY_ENABLED: ${TELEMETRY_ENABLED}
      # client side
      NEXT_PUBLIC_USER_UUID: ${USER_UUID}
      NEXT_PUBLIC_POSTHOG_API_KEY: ${POSTHOG_API_KEY}
      NEXT_PUBLIC_POSTHOG_HOST: ${POSTHOG_HOST}
      NEXT_PUBLIC_TELEMETRY_ENABLED: ${TELEMETRY_ENABLED}
      EXPERIMENTAL_ENGINE_RUST_VERSION: ${EXPERIMENTAL_ENGINE_RUST_VERSION}
      # configs
      WREN_PRODUCT_VERSION: ${WREN_PRODUCT_VERSION}
    ports:
      # HOST_PORT is the port you want to expose to the host machine
      - ${HOST_PORT}:3000
    volumes:
      - ./data/wren-ui:/app/data
    networks:
      - app-network
    depends_on:
      - wren-ai-service
      - wren-engine